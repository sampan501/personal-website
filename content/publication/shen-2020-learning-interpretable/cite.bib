@misc{shen2020-LearningInterpretable,
 abstract = {Decision forests are popular tools for classification and regression. These forests naturally produce proximity matrices measuring how often each pair of observations lies in the same leaf node. It has been demonstrated that these proximity matrices can be thought of as kernels, connecting the decision forest literature to the extensive kernel machine literature. While other kernels are known to have strong theoretical properties such as being characteristic, no similar result is available for any decision forest based kernel. In this manuscript,we prove that the decision forest induced proximity can be made characteristic, which can be used to yield a universally consistent statistic for testing independence. We demonstrate the performance of the induced kernel on a suite of 20 high-dimensional independence test settings. We also show how this learning kernel offers insights into relative feature importance. The decision forest induced kernel typically achieves substantially higher testing power than existing popular methods in statistical tests.},
 archiveprefix = {arXiv},
 author = {Shen, Cencheng and Panda, Sambit and Vogelstein, Joshua T.},
 copyright = {All rights reserved},
 doi = {10.48550/arXiv.1812.00029},
 eprint = {1812.00029},
 eprinttype = {arxiv},
 month = {September},
 number = {arXiv:1812.00029},
 primaryclass = {cs, stat},
 publisher = {arXiv},
 title = {Learning Interpretable Characteristic Kernels via Decision Forests},
 year = {2020}
}

